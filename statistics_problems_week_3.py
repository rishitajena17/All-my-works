# -*- coding: utf-8 -*-
"""Statistics Problems Week-3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t6yqlu6Nq6ZXOh0-KoWRABUl4HimahA7

## ***SECTION-1***

**1. Calculate the mean, median, mode, standard deviation, range, and quartile for all columns?**
"""

from google.colab import files

uploaded = files.upload()
import pandas as pd


df = pd.read_csv('Campaign_data_statistics_section_1.csv')

import pandas as pd


df = pd.read_csv('Campaign_data_statistics_section_1.csv')


numeric_columns = df.select_dtypes(include=[float, int])


mean_values = numeric_columns.mean()


median_values = numeric_columns.median()


mode_values = numeric_columns.mode().iloc[0]


std_values = numeric_columns.std()


range_values = numeric_columns.max() - numeric_columns.min()


quartile_values = numeric_columns.quantile([0.25, 0.5, 0.75])


print("Mean:")
print(mean_values)
print("\nMedian:")
print(median_values)
print("\nMode:")
print(mode_values)
print("\nStandard Deviation:")
print(std_values)
print("\nRange:")
print(range_values)
print("\nQuartiles:")
print(quartile_values)

"""**2-Transform the [Click_Rate] series into z-score and max-min.**"""

from google.colab import files
import pandas as pd


df = pd.read_csv('Campaign_data_statistics_section_1.csv')


print("Column Names:", df.columns)


print(df.head())

click_rate_series = df['click_rate']


z_score = (click_rate_series - click_rate_series.mean()) / click_rate_series.std()


max_min_scaling = (click_rate_series - click_rate_series.min()) / (click_rate_series.max() - click_rate_series.min())


df['Click_Rate_Z_Score'] = z_score
df['Click_Rate_Max_Min'] = max_min_scaling

"""**3-Prepare boxplot and count the number of outliers present out of IQR. [Click_Rate]**"""

from google.colab import files
import pandas as pd


df = pd.read_csv('Campaign_data_statistics_section_1.csv')
import matplotlib.pyplot as plt


plt.figure(figsize=(8, 6))
plt.boxplot(click_rate_series)
plt.title('Boxplot of Click_Rate')
plt.show()


q1 = click_rate_series.quantile(0.25)
q3 = click_rate_series.quantile(0.75)
iqr = q3 - q1


lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
num_outliers = ((click_rate_series < lower_bound) | (click_rate_series > upper_bound)).sum()

print("Number of outliers:", num_outliers)

"""**4-Study the linear relationship among all columns using scatter plot and find out high linearly correlated columns.**

"""

from google.colab import files
import pandas as pd


df = pd.read_csv('Campaign_data_statistics_section_1.csv')
import seaborn as sns


numeric_columns = df.select_dtypes(include=[float, int])


sns.pairplot(df, vars=numeric_columns)
plt.show()


correlation_matrix = df.corr()

highly_correlated_cols = correlation_matrix['click_rate'][abs(correlation_matrix['click_rate']) > 0.5].index.tolist()
print("Highly correlated columns with Click_Rate:", highly_correlated_cols)

"""**5-Non-linear data can be tricky for the Pearson correlation coefficient to estimate relationship therefore evaluate the Spearman correlation coefficient and find out high correlated columns?**"""

from google.colab import files
import pandas as pd

df = pd.read_csv('Campaign_data_statistics_section_1.csv')
spearman_corr_matrix = df.corr(method='spearman')

highly_spearman_corr_cols = spearman_corr_matrix['click_rate'][abs(spearman_corr_matrix['click_rate']) > 0.5].index.tolist()
print("Highly Spearman correlated columns with Click_Rate:", highly_spearman_corr_cols)

"""**6- Check whether the series given in the data column [Click_Rate] is following the normality assumption using any two methods and transform the series into normal distribution if series is not normal?**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro, anderson
import numpy as np

df = pd.read_csv('Campaign_data_statistics_section_1.csv')
click_rate_series = df['click_rate']
transformed_click_rate_series = click_rate_series.apply(lambda x: np.log(x))
shapiro_stat_transformed, shapiro_p_value_transformed = shapiro(transformed_click_rate_series)
print("Shapiro-Wilk Test p-value (transformed):", shapiro_p_value_transformed)

"""## ***SECTION-2***

**A probability distribution is a mathematical function that represents the probability of an event. Many processes occurring in the world around us can be explained by a small number of well-studied and analysed distributions.
Hypothesis testing is a crucial procedure to perform when you want to make inferences about a population using a random sample. These inferences include estimating population properties such as the mean, differences between means, proportions, and the relationships between variables.**

***1-X is normally distributed, and the mean is 12 and standard deviation is 4.Find out the probability of the following:
(i) P( X >= 20 ) (ii) P( X <= 20 ) (iii) P( 0 <= X <= 12)***
"""

import scipy.stats as stats

mean = 12
std_dev = 4


z_score = (20 - mean) / std_dev
prob_x_greater_than_20 = 1 - stats.norm.cdf(z_score)
print("P(X >= 20) =", prob_x_greater_than_20)


prob_x_less_than_20 = stats.norm.cdf(z_score)
print("P(X <= 20) =", prob_x_less_than_20)


z_score_0 = (0 - mean) / std_dev
prob_x_less_than_0 = stats.norm.cdf(z_score_0)

prob_x_between_0_and_12 = prob_x_less_than_20 - prob_x_less_than_0
print("P(0 <= X <= 12) =", prob_x_between_0_and_12)

"""***2-The mean yield for one-acre plot is 662 kilos with a s.d. 32 kilos. Assuming normal distribution, how many one-acre plots in a batch of 1000 plots would you expect to have yield.
(i) over 700 kilos (ii) below 650 kilos, and (iii) what is the lowest yield of the best 100 plots?***
"""

import scipy.stats as stats

mean = 662
std_dev = 32
batch_size = 1000

z_score = (700 - mean) / std_dev
prob_over_700 = 1 - stats.norm.cdf(z_score)
expected_plots_over_700 = prob_over_700 * batch_size
print("Number of plots with yield over 700 kilos:", expected_plots_over_700)

z_score_650 = (650 - mean) / std_dev
prob_below_650 = stats.norm.cdf(z_score_650)
expected_plots_below_650 = prob_below_650 * batch_size
print("Number of plots with yield below 650 kilos:", expected_plots_below_650)

z_score_best_100 = stats.norm.ppf(1 - 0.01)
lowest_yield_best_100 = mean + z_score_best_100 * std_dev
print("Lowest yield of the best 100 plots:", lowest_yield_best_100)

"""***3-ten coins are thrown simultaneously. Find the probability of getting at least seven heads?***

"""

from math import comb

def binomial_probability(n, k, p):
    return comb(n, k) * (p ** k) * ((1 - p) ** (n - k))

n_coins = 10
p_head = 0.5
probability_at_least_seven_heads = sum(binomial_probability(n_coins, k, p_head) for k in range(7, n_coins + 1))

print("Probability of getting at least seven heads:", probability_at_least_seven_heads)

"""***4-A car hire firm has five cars which it fires out day by day. The number of demands for a car on each day is distributed as Poisson variate with mean 2 Calculate the proportion
of days on which
(i) neither car is used.
(ii) two demand is there.
(iii) all car used.***

"""

import math


mean_demands = 2


probability_zero_demands = math.exp(-mean_demands)
print("Probability of neither car being used:", probability_zero_demands)



probability_two_demands = (math.exp(-mean_demands) * (mean_demands ** 2)) / math.factorial(2)
print("Probability of two demands:", probability_two_demands)


probability_all_cars_used = (math.exp(-mean_demands) * (mean_demands ** 5)) / math.factorial(5)
print("Probability of all cars being used:", probability_all_cars_used)

"""***5-An engineer works for Coca Cola. He knows that the filling machine has a variance of 6 cl. (the filling process can be approximated by a Normal Distribution).Knowing this, he sets the machine to a target fill of 348 cl. In a routine check with 25 cans, he measures an average of 345 cl. Is it possible that the machine is malfunctioning?***

"""

import numpy as np
from scipy.stats import t


target_fill = 348
sample_mean = 345
sample_variance = 6
sample_size = 25


sample_std_dev = np.sqrt(sample_variance)


t_statistic = (sample_mean - target_fill) / (sample_std_dev / np.sqrt(sample_size))


degrees_of_freedom = sample_size - 1


alpha = 0.05


critical_t_value = t.ppf(1 - alpha / 2, df=degrees_of_freedom)


if abs(t_statistic) > critical_t_value:
    print("The machine is likely malfunctioning.")
else:
    print("There is not enough evidence to suggest that the machine is malfunctioning.")

"""***6. A random sample of corporate employees of a firm was selected and asked their opinions about HR department polices. The same number of each gender was included within each domain group. Test the hypothesis at 5% level that opinions are independent of the domain groupings***"""

import numpy as np
import scipy.stats as stats


observed_data = np.array([[120, 80], [130, 70], [70, 30], [80, 20]])


row_totals = observed_data.sum(axis=1)


column_totals = observed_data.sum(axis=0)


grand_total = observed_data.sum()

expected_data = np.outer(row_totals, column_totals) / grand_total


chi2_statistic = np.sum((observed_data - expected_data) ** 2 / expected_data)


degrees_of_freedom = (observed_data.shape[0] - 1) * (observed_data.shape[1] - 1)


alpha = 0.05


critical_chi2_value = stats.chi2.ppf(1 - alpha, df=degrees_of_freedom)


if chi2_statistic > critical_chi2_value:
    print("Opinions are dependent on domain groupings (reject H0).")
else:
    print("Opinions are independent of domain groupings (fail to reject H0).")